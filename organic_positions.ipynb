{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the experiment on [memorization](https://colab.research.google.com/drive/1mcHTLZIWPs1n4V5CmdJxMSYN6_KFZ0Y4?usp=sharing)\n",
    "I chose a symmteric matrix, because the transformer has no positional encoding and thus can't learn a non-symmetric matrix.\n",
    "\n",
    "However, in the original experiments I did with a very slightly different transformer architecture, the model was able to learn even a non-symmetric matrix, \n",
    "and this phenomenon also happened with the final transformer architecture with a 2-layer network. This phenomenon was first observed in larger models in [Haviv et al. 2022](https://arxiv.org/abs/2203.16634).\n",
    "\n",
    "The difference in the model architecture is that the model in the original experiment was predicting the series $(j, M_{ij})$ from $(i, j)$, while the model in the linked notebook is predicting the series $(i, j, M_{ij})$ from $(i, j)$ by the addition of a third sequence item that's initialized to the zero vector.\n",
    "\n",
    "The difference is that in the first model, the residual stream of the last token is initialized with the embedding of $j$. The model is therefore able to distinguish between a \"1\" that was passed from the attention, and a \"1\" that was directly there in the residual stream. In the second model, the residual stream is initialized with the zero vector, and the model receives both positions from the attention, so it's unable to distinguish between the two.\n",
    "\n",
    "We now show how even in the second case, a 2-layer model can still learn positional information. The intuitive explanation is that in the second layer, the second token already contains information that was passed from the first token through the attention, and the model learns to distinguish between a $(1, )$ and a $(*, 1)$ because the latter contains \"extra information\" from the first token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 17,  8, 24, 26, 12,  4, 12, 13, 17, 16, 18,  6, 22,  2,  6,  6, 27, 14, 17, 13,  8, 22, 14, 26,  6, 21, 27, 27,  7],\n",
       "        [ 9, 20,  2,  2, 29, 10, 16, 14,  2, 10, 12,  9, 21, 15, 27, 17, 23,  4, 14,  2, 14, 11,  0,  5,  7, 23, 25,  3, 18, 28],\n",
       "        [21,  3, 26, 25,  6,  7,  7, 15, 22, 19, 18, 20, 20, 15, 27,  1, 20, 11,  3, 20, 10, 10, 28, 29, 10, 10, 16,  7, 10, 25],\n",
       "        [ 9,  9, 22, 29,  3, 17, 10, 11, 23,  2,  2,  6, 14,  6, 17, 14, 26, 14,  2, 21, 22,  0,  9, 17, 22,  9, 27, 21,  6, 12],\n",
       "        [ 4,  3,  1,  6, 17, 12,  1,  4, 18, 27, 10, 12, 29, 24, 21, 15, 25,  4, 14, 17,  1, 22, 17, 29, 16, 13,  5, 25, 19,  0],\n",
       "        [ 5, 15,  0, 14,  2, 12,  0, 28, 17, 14, 12, 22, 15,  8, 16, 20, 18, 10,  0,  1, 19, 22, 23, 27, 18,  6, 14,  9, 29,  7],\n",
       "        [ 3,  0, 14, 15,  1, 24,  6, 28, 26,  4, 20, 26,  2,  6,  5, 22,  6, 24, 18, 10,  7,  9,  7, 10, 25, 16, 17, 29,  4, 24],\n",
       "        [12, 17, 21,  4,  2, 23, 22, 20, 14, 22, 23, 13, 21,  2, 12,  0,  0, 10, 20, 16,  0, 26,  5,  6, 20,  2, 25, 26, 28, 18],\n",
       "        [ 5, 25, 12,  5, 22, 15, 17,  7,  7, 20,  0, 11, 10, 14,  2,  4, 12,  7, 10, 26, 18, 24,  3, 14, 29, 17,  0, 18,  3, 21],\n",
       "        [26, 18,  8, 11,  6, 17, 23, 12,  5, 16, 17, 29,  8, 20, 17, 28, 23, 13,  3, 25, 20,  7, 19, 16, 23, 11, 16,  8, 14, 24],\n",
       "        [12, 21,  7, 26, 28, 14,  3, 15, 27, 17,  9, 14,  7, 18, 16, 29, 18, 13, 10, 11, 24, 24, 21, 11, 19,  9, 14, 12, 29, 20],\n",
       "        [23, 23,  4,  5, 22,  6,  3, 22, 12, 16, 13,  2, 20, 20, 20, 21, 23, 21,  2, 10, 28, 25,  1, 23, 28,  8, 23, 29, 19,  6],\n",
       "        [25, 25, 11,  8, 25, 22, 11, 20, 14, 27,  8, 15, 26, 23, 19, 12, 18,  2,  8, 14, 28,  2, 17, 18, 19, 23, 12, 13, 21, 10],\n",
       "        [25, 27, 28, 10,  5, 23, 17,  8, 12,  5, 19, 13, 20, 25,  7,  9, 12,  6, 10, 24, 25,  7,  3, 12, 13, 21, 26, 17, 13,  8],\n",
       "        [27,  3, 28, 21,  4, 20, 25, 16,  4, 13,  9, 29,  1,  0, 19,  9, 19, 11, 12, 27, 14, 26,  8, 11, 12,  2, 26,  9, 18, 23],\n",
       "        [ 7, 19, 27, 18, 27,  1,  8,  8,  8,  5, 24, 21, 29,  2,  3,  9, 13,  8, 11,  8,  8, 22, 14, 21,  5,  6,  9, 21, 19, 23],\n",
       "        [22,  1,  8,  2, 12,  0, 14, 16,  2, 24, 23,  4, 22,  6,  0,  7, 23,  1,  1, 10,  3, 16, 23, 17, 19, 19,  5, 21, 15,  4],\n",
       "        [26, 11, 18, 27,  3, 15, 20, 18, 12,  2, 28, 15,  0,  2,  1,  5,  0,  0, 15, 25, 11,  8, 18, 27, 29,  2, 29, 26,  8, 11],\n",
       "        [ 1, 24, 23, 15, 23, 19, 13, 29, 16,  9,  1,  7,  6, 20, 24,  9, 22,  7, 25, 18,  8, 23,  0,  8,  5,  3,  7, 25, 21, 20],\n",
       "        [25, 16, 10, 28,  2, 11, 12, 19, 27, 29,  9, 11, 29,  2, 13,  0,  4,  9, 20, 17, 19, 10, 15,  1,  7, 13,  6,  9, 25, 29],\n",
       "        [ 1, 21,  4,  0, 27, 25, 26, 29, 23, 29,  8, 24, 10,  6, 18, 12, 21, 13,  8, 10, 12,  7, 22,  0, 21,  9, 20, 24,  5,  0],\n",
       "        [14, 25, 14,  1,  2, 25, 18, 20,  5, 28, 13,  5, 14, 14, 11, 24, 28,  1, 12, 21, 14, 28, 16,  7,  2, 14,  8, 19, 23, 12],\n",
       "        [23,  7, 12, 23, 16,  5,  3, 10, 18, 11, 13, 12, 17,  4,  9,  3,  2,  1, 16, 12,  0, 18, 23,  3, 26, 19, 15, 12, 11, 17],\n",
       "        [18, 21,  4,  4, 26,  4, 22, 10, 13, 24, 17, 10,  6, 23, 17, 12, 15, 23,  1,  7, 24, 27,  4, 13,  3, 18,  2, 29,  5, 11],\n",
       "        [ 1,  4, 27, 19, 11, 24, 21, 21, 26,  8,  3, 21, 26, 24,  5,  0, 15,  6, 25, 29, 25, 27,  6, 21, 24,  3, 15, 17,  9, 23],\n",
       "        [ 3,  6,  1, 29, 10,  4, 12, 27,  2,  8, 13,  1, 19, 13,  0, 18,  3, 16, 27, 14,  6, 10,  1,  2, 27, 15, 28,  1, 17, 18],\n",
       "        [28, 23, 22, 16, 26, 17, 29,  9, 26,  2, 20, 17,  5,  1,  8, 11,  0,  5, 26,  7, 12, 19, 26, 22, 15, 11,  3, 20, 18, 24],\n",
       "        [15, 17,  8, 16, 14, 16,  0, 15,  1, 10, 13, 18,  3, 25,  2,  1, 19, 23, 17, 23,  7,  8, 29,  4,  1,  1, 27,  1, 10, 19],\n",
       "        [ 9, 27, 22, 28, 13, 26,  1,  5, 29, 24, 15, 11, 14, 22, 29, 22,  3, 12, 25, 23, 21, 14, 26, 28, 23, 20, 13,  6, 10, 16],\n",
       "        [21, 10, 10, 23,  4, 13, 29, 15, 15, 16, 18, 11, 14, 24, 19,  7, 20,  4,  5, 26,  9, 29,  8,  3, 29, 16, 15,  7,  1, 18]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(precision=2, sci_mode=False, linewidth=200)\n",
    "n_tokens = 30\n",
    "memory = torch.randint(0, n_tokens, (n_tokens, n_tokens))\n",
    "memory = memory.long()\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, layers, d_model, n_head, dim_feedforwrad, padding=True):\n",
    "        super().__init__()\n",
    "        self.tokens = list(range(n_tokens))\n",
    "        self.padding = padding\n",
    "        self.embed = nn.Embedding(n_tokens, d_model)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, dim_feedforward=dim_feedforwrad, batch_first=True, dropout=0),\n",
    "            num_layers=layers\n",
    "        )\n",
    "        self.unembed = nn.Linear(d_model, n_tokens)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        if self.padding:\n",
    "            # pad x along the sequence dimension\n",
    "            x = nn.functional.pad(x, (0, 0, 0, 1), mode='constant', value=0)\n",
    "\n",
    "        attn_mask = torch.tril(torch.ones(x.shape[1], x.shape[1]), diagonal=0).to(self.device)\n",
    "        x = self.encoder(x, is_causal=True, mask=attn_mask)\n",
    "        x = self.unembed(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, lr=1e-3, batch_size=128, n_epochs=1000):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for _ in tqdm(range(n_epochs)):\n",
    "            batch = self.generate_data(batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            input = batch[:, :-1]\n",
    "            output = self(input)\n",
    "            if not self.padding:\n",
    "                batch = batch[:, 1:]\n",
    "            loss = criterion(output.reshape(-1, len(self.tokens)), batch.reshape(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('loss: ', loss.item())\n",
    "    \n",
    "    def generate_data(self, batch_size):\n",
    "        random_indices = torch.randint(0, n_tokens, (batch_size, 2))  # [batch_size, 2]\n",
    "        next_tokens = memory[random_indices[:, 0], random_indices[:, 1]].unsqueeze(1)  # [batch_size, 1]\n",
    "        tensor = torch.cat([random_indices, next_tokens], dim=1)\n",
    "        return tensor.to(self.device)\n",
    "\n",
    "    def evaluate(self):\n",
    "        samples = 1000\n",
    "        data = self.generate_data(samples)\n",
    "        output = (self(data[:,:-1])[:,-1,:].argmax(dim=-1))\n",
    "        print('Accuracy: ', output.eq(data[:,-1]).sum().item() / samples)\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:25<00:00, 118.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.3708096742630005\n",
      "Accuracy:  0.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(layers=1, d_model=16, n_head=2, dim_feedforwrad=64).to(device)\n",
    "model.train(lr=1e-2, n_epochs=3000, batch_size=300)\n",
    "model.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's slightly more than 0.5 because the model is able to learn the diagonal. The theoretical accuracy is $0.5 + \\frac{\\mathrm{n\\_tokens}}{\\mathrm{n\\_tokens}^2} = 0.5 + \\frac{30}{900} = 0.53\\overline{3}$. might be slightly more because some elements are symmetric by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:39<00:00, 75.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.031408943235874176\n",
      "Accuracy:  0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(layers=2, d_model=16, n_head=2, dim_feedforwrad=64).to(device)\n",
    "model.train(lr=1e-2, n_epochs=3000, batch_size=300)\n",
    "model.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show that without the zero-padding, even a 1-layer model can learn the positional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:20<00:00, 143.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.9394631385803223\n",
      "Accuracy:  0.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(layers=1, padding=False, d_model=16, n_head=2, dim_feedforwrad=64).to(device)\n",
    "model.train(lr=1e-2, n_epochs=3000, batch_size=300)\n",
    "model.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

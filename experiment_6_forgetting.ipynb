{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 5, 2, 7, 0, 5, 8, 2, 5],\n",
       "        [0, 0, 1, 4, 0, 0, 0, 5, 6, 2],\n",
       "        [5, 1, 8, 9, 4, 9, 1, 0, 2, 2],\n",
       "        [2, 4, 9, 8, 4, 3, 8, 4, 9, 6],\n",
       "        [7, 0, 4, 4, 8, 1, 6, 4, 0, 1],\n",
       "        [0, 0, 9, 3, 1, 0, 8, 6, 2, 3],\n",
       "        [5, 0, 1, 8, 6, 8, 6, 2, 4, 8],\n",
       "        [8, 5, 0, 4, 4, 6, 2, 1, 0, 0],\n",
       "        [2, 6, 2, 9, 0, 2, 4, 0, 3, 6],\n",
       "        [5, 2, 2, 6, 1, 3, 8, 0, 6, 9]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, math\n",
    "torch.set_printoptions(precision=2, sci_mode=False, linewidth=200)\n",
    "def generate_memory(n_tokens):\n",
    "    # memory = torch.randint(0, n_tokens, (n_tokens, n_tokens))\n",
    "    memory = torch.triu(torch.ones(n_tokens, n_tokens), diagonal=0)\n",
    "    memory = memory * torch.randint_like(memory, 0, n_tokens)\n",
    "    memory = memory + memory.T - torch.diag(memory.diagonal())\n",
    "    memory = memory.long()\n",
    "    return memory\n",
    "\n",
    "generate_memory(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_head, batch_first=True, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, tgt, skip_feedforward=False, skip_self_attn=False, linear_mask=None, history=None, custom_attention=False, head_mask=None):\n",
    "        hist = {}\n",
    "        attn_heads = None\n",
    "        if not skip_self_attn:\n",
    "            # mask = torch.triu(torch.ones(tgt.shape[1], tgt.shape[1]), diagonal=1).bool().to(self.device)\n",
    "            # tgt2, attn_heads = hist['self_attn_non_residual'] = self.self_attn(tgt, tgt, tgt, attn_mask=mask, average_attn_weights=False)\n",
    "            tgt2 = hist['self_attn_non_residual'] = self.attn_forward(tgt, self.self_attn, custom_attention, hist, head_mask)\n",
    "            tgt = hist['self_attn'] = tgt + tgt2\n",
    "        tgt = hist['norm1'] = self.norm1(tgt)\n",
    "        if self.dim_feedforward > 0 and not skip_feedforward:\n",
    "            tgt2 = hist['linear1'] = nn.functional.relu(self.linear1(tgt))\n",
    "            tgt2 = hist['linear1_dropout'] = self.dropout(tgt2)\n",
    "            if linear_mask is not None:\n",
    "                tgt2 = tgt2 * linear_mask\n",
    "            tgt2 = hist['linear2_non_residual'] = self.linear2(tgt2)\n",
    "            tgt = hist['linear2'] = tgt + tgt2\n",
    "        tgt = hist['norm2'] = self.norm2(tgt)\n",
    "        return tgt if history is None else hist[history], attn_heads\n",
    "    \n",
    "    def attn_forward(self, x, attn, custom_attention, history, head_mask=None):\n",
    "        attn_mask = torch.tril(torch.ones(x.shape[1], x.shape[1]), diagonal=0).to(self.device)\n",
    "        if not custom_attention:\n",
    "            return attn(x, x, x, attn_mask=attn_mask)[0]\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.transpose(0, 1)\n",
    "        # this is just torch's attention but expanded so we can modify it\n",
    "        proj = F.linear(x, attn.in_proj_weight, attn.in_proj_bias)\n",
    "        proj = proj.unflatten(-1, (3, self.d_model)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "        q, k, v = proj[0], proj[1], proj[2]\n",
    "        q = q.unflatten(-1, (self.n_head, self.d_model // self.n_head)).permute(1, 2, 0, 3)\n",
    "        k = k.unflatten(-1, (self.n_head, self.d_model // self.n_head)).permute(1, 2, 0, 3)\n",
    "        v = v.unflatten(-1, (self.n_head, self.d_model // self.n_head)).permute(1, 2, 0, 3)\n",
    "        \n",
    "        history.update({\n",
    "            'q': q,\n",
    "            'k': k,\n",
    "            'v': v\n",
    "        })\n",
    "\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == False, float('-inf'))\n",
    "        \n",
    "        attn_output = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1)) + attn_mask\n",
    "        attn_output = F.softmax(attn_output, dim=-1)\n",
    "        attn_output = torch.matmul(attn_output, v)\n",
    "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous()  # [seq_len, batch_size, n_head, d_model // n_head]\n",
    "        # apply head mask\n",
    "        if head_mask is not None:\n",
    "            attn_output = attn_output * head_mask[None, None, :, None]\n",
    "        \n",
    "        history['attn_output'] = attn_output\n",
    "\n",
    "        attn_output = attn_output.flatten(-2, -1)\n",
    "        attn_output = F.linear(attn_output, attn.out_proj.weight, attn.out_proj.bias)\n",
    "        return attn_output.transpose(0, 1)\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_head, hidden_size, n_tokens, max_len, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tokens = list(range(n_tokens))\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.embed = nn.Embedding(n_tokens, embedding_dim=d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model=d_model, n_head=n_head, dim_feedforward=hidden_size)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.unembed = nn.Linear(d_model, n_tokens)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        skip_feedforward=False,\n",
    "        skip_self_attn=False,\n",
    "        return_before_embedding=False,\n",
    "        linear_mask=None,\n",
    "        history=None,\n",
    "        return_attn_weights=False,\n",
    "        custom_attention=False,\n",
    "        head_mask=None):\n",
    "        if head_mask is not None:\n",
    "            custom_attention = True\n",
    "        tgt = self.embed(x)\n",
    "        tgt = F.pad(tgt, (0, 0, 0, 1))  # [batch_size, seq_len + 1, d_model]\n",
    "        for layer in self.layers:\n",
    "            tgt, attn_heads = layer(\n",
    "                tgt,\n",
    "                skip_feedforward=skip_feedforward,\n",
    "                skip_self_attn=skip_self_attn,\n",
    "                linear_mask=linear_mask,\n",
    "                history=history,\n",
    "                custom_attention=custom_attention,\n",
    "                head_mask=head_mask)\n",
    "            if history is not None:\n",
    "                return tgt\n",
    "        if return_before_embedding:\n",
    "            return tgt\n",
    "        x = self.unembed(tgt)\n",
    "        if return_attn_weights:\n",
    "            return x, attn_heads\n",
    "        return x\n",
    "\n",
    "    def train(self, memory, episode, episode_length, lr=1e-3, batch_size=128, n_epochs=1000, verbose=True):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for _ in tqdm(range(n_epochs)) if verbose else range(n_epochs):\n",
    "            batch = self.generate_data(batch_size, memory, episode, episode_length)\n",
    "            optimizer.zero_grad()\n",
    "            input = batch[:, :-1]\n",
    "            output = self(input)\n",
    "            loss = criterion(output.reshape(-1, len(self.tokens)), batch.reshape(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if verbose:\n",
    "            print('loss: ', loss.item())\n",
    "\n",
    "    def generate_data(self, batch_size, memory, episode, episode_length):\n",
    "        random_indices1 = torch.randint(episode * episode_length, (episode + 1) * episode_length, (batch_size, 1))  # [batch_size, 1]\n",
    "        random_indices2 = torch.randint(0, memory.shape[1], (batch_size, 1))  # [batch_size, 1]\n",
    "        random_indices = torch.cat([random_indices1, random_indices2], dim=1)\n",
    "        \n",
    "        next_tokens = memory[random_indices[:, 0], random_indices[:, 1]].unsqueeze(1)  # [batch_size, 1]\n",
    "        tensor = torch.cat([random_indices, next_tokens], dim=1)\n",
    "        return tensor.to(self.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  0.01\n",
      "accuracy  0.021\n",
      "accuracy  0.028\n",
      "accuracy  0.048\n",
      "accuracy  0.074\n",
      "accuracy  0.118\n",
      "accuracy  0.16\n",
      "accuracy  0.24\n",
      "accuracy  0.27\n",
      "accuracy  0.352\n",
      "accuracy  0.469\n",
      "accuracy  0.479\n",
      "accuracy  0.554\n",
      "accuracy  0.673\n",
      "accuracy  0.72\n",
      "accuracy  0.788\n",
      "accuracy  0.874\n",
      "accuracy  0.894\n",
      "accuracy  0.919\n",
      "accuracy  0.96\n",
      "accuracy  0.981\n",
      "accuracy  0.992\n",
      "accuracy  0.988\n",
      "accuracy  0.998\n",
      "accuracy  1.0\n",
      "Finished training in 24 epochs\n",
      "model accuracy on episode 0:  1.0\n",
      "accuracy  0.018\n",
      "accuracy  0.049\n",
      "accuracy  0.081\n",
      "accuracy  0.099\n",
      "accuracy  0.181\n",
      "accuracy  0.229\n",
      "accuracy  0.303\n",
      "accuracy  0.388\n",
      "accuracy  0.458\n",
      "accuracy  0.528\n",
      "accuracy  0.596\n",
      "accuracy  0.648\n",
      "accuracy  0.693\n",
      "accuracy  0.767\n",
      "accuracy  0.792\n",
      "accuracy  0.835\n",
      "accuracy  0.865\n",
      "accuracy  0.926\n",
      "accuracy  0.95\n",
      "accuracy  0.958\n",
      "accuracy  0.981\n",
      "accuracy  0.995\n",
      "accuracy  0.994\n",
      "accuracy  0.995\n",
      "accuracy  0.999\n",
      "accuracy  0.998\n",
      "accuracy  1.0\n",
      "Finished training in 26 epochs\n",
      "model accuracy on episode 0:  0.008\n",
      "model accuracy on episode 1:  1.0\n",
      "accuracy  0.02\n",
      "accuracy  0.026\n",
      "accuracy  0.056\n",
      "accuracy  0.113\n",
      "accuracy  0.13\n",
      "accuracy  0.22\n",
      "accuracy  0.272\n",
      "accuracy  0.311\n",
      "accuracy  0.387\n",
      "accuracy  0.394\n",
      "accuracy  0.499\n",
      "accuracy  0.531\n",
      "accuracy  0.601\n",
      "accuracy  0.644\n",
      "accuracy  0.683\n",
      "accuracy  0.744\n",
      "accuracy  0.784\n",
      "accuracy  0.819\n",
      "accuracy  0.843\n",
      "accuracy  0.881\n",
      "accuracy  0.894\n",
      "accuracy  0.914\n",
      "accuracy  0.948\n",
      "accuracy  0.956\n",
      "accuracy  0.969\n",
      "accuracy  0.982\n",
      "accuracy  0.989\n",
      "accuracy  0.997\n",
      "accuracy  0.99\n",
      "accuracy  0.998\n",
      "accuracy  0.999\n",
      "accuracy  1.0\n",
      "Finished training in 31 epochs\n",
      "model accuracy on episode 0:  0.006\n",
      "model accuracy on episode 1:  0.003\n",
      "model accuracy on episode 2:  1.0\n",
      "accuracy  0.019\n",
      "accuracy  0.023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9863/3460884310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m144\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9863/3883772721.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, memory, episode, episode_length, lr, batch_size, n_epochs, verbose)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "n_tokens = 320\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = ToyTransformer(n_layers=3, d_model=16, n_head=4, hidden_size=hidden_size, n_tokens=n_tokens, max_len=3, dropout=0.1).to(device)\n",
    "memory = generate_memory(n_tokens)\n",
    "history = []\n",
    "episode_length = 320 // 64\n",
    "for episode in range(64):\n",
    "    for epoch in range(999):\n",
    "        model.train(memory, episode, episode_length, lr=1e-3, n_epochs=100, batch_size=144 * 2, verbose=False)\n",
    "        samples = 1000\n",
    "        data = model.generate_data(samples, memory, episode, episode_length)\n",
    "        output = (model(data[:,:-1])[:,-1,:].argmax(dim=-1))\n",
    "        accuracy = output.eq(data[:,-1]).sum().item() / samples\n",
    "        print('accuracy ', accuracy)\n",
    "        if accuracy == 1.0:\n",
    "            print(f'Finished training in {epoch} epochs')\n",
    "            history.append(memory)\n",
    "            memory = generate_memory(n_tokens)\n",
    "\n",
    "            for ep, mem in enumerate(history):\n",
    "                data = model.generate_data(samples, mem, ep, episode_length)\n",
    "                output = (model(data[:,:-1])[:,-1,:].argmax(dim=-1))\n",
    "                accuracy = output.eq(data[:,-1]).sum().item() / samples\n",
    "                print(f'model accuracy on episode {ep}: ', accuracy)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
